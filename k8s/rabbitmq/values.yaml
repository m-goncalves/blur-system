## Bitnami RabbitMQ image version
## ref: https://hub.docker.com/r/bitnami/rabbitmq/tags/
##
image:
  registry: docker.io
  repository: bitnami/rabbitmq
  tag: 3.8.9-debian-10-r37
  debug: false
  pullPolicy: IfNotPresent
clusterDomain: cluster.local
auth:
  existingPasswordSecret: rabbit
  existingErlangSecret: rabbit
  tls:
    enabled: false
    failIfNoPeerCert: true
    sslOptionsVerify: verify_peer
    caCertificate: |-
    serverCertificate: |-
    serverKey: |-
    # existingSecret: name-of-existing-secret-to-rabbitmq
logs: '-'
ulimitNofiles: '65536'
memoryHighWatermark:
  enabled: true
  type: 'relative'
  value: 0.4
plugins: 'rabbitmq_management rabbitmq_peer_discovery_k8s'
extraPlugins: 'rabbitmq_auth_backend_ldap'
clustering:
  addressType: hostname
  rebalance: false

  ## forceBoot: executes 'rabbitmqctl force_boot' to force boot cluster shut down unexpectedly in an
  ## unknown order.
  ## ref: https://www.rabbitmq.com/rabbitmqctl.8.html#force_boot
  ##
  forceBoot: false
loadDefinition:
  enabled: false
terminationGracePeriodSeconds: 120
extraEnvVars: []
extraContainerPorts: []
configuration: |-
  ## Username and password
  default_user = {{ .Values.auth.username }}
  default_pass = {{ .Values.auth.username }}
  ## Clustering
  cluster_formation.peer_discovery_backend  = rabbit_peer_discovery_k8s
  cluster_formation.k8s.host = kubernetes.default.svc.{{ .Values.clusterDomain }}
  cluster_formation.node_cleanup.interval = 10
  cluster_formation.node_cleanup.only_log_warning = true
  cluster_partition_handling = autoheal
  # queue master locator
  queue_master_locator = min-masters
  # enable guest user
  loopback_users.guest = false
  {{ tpl .Values.extraConfiguration . }}
  {{- if .Values.auth.tls.enabled }}
  ssl_options.verify = {{ .Values.auth.tls.sslOptionsVerify }}
  listeners.ssl.default = {{ .Values.service.tlsPort }}
  ssl_options.fail_if_no_peer_cert = {{ .Values.auth.tls.failIfNoPeerCert }}
  ssl_options.cacertfile = /opt/bitnami/rabbitmq/certs/ca_certificate.pem
  ssl_options.certfile = /opt/bitnami/rabbitmq/certs/server_certificate.pem
  ssl_options.keyfile = /opt/bitnami/rabbitmq/certs/server_key.pem
  {{- end }}
  {{- if .Values.ldap.enabled }}
  auth_backends.1 = rabbit_auth_backend_ldap
  auth_backends.2 = internal
  {{- range $index, $server := .Values.ldap.servers }}
  auth_ldap.servers.{{ add $index 1 }} = {{ $server }}
  {{- end }}  auth_ldap.port = {{ .Values.ldap.port }}
  auth_ldap.user_dn_pattern = {{ .Values.ldap.user_dn_pattern  }}
  {{- if .Values.ldap.tls.enabled }}
  auth_ldap.use_ssl = true
  {{- end }}
  {{- end }}
  {{- if .Values.metrics.enabled }}
  ## Prometheus metrics
  prometheus.tcp.port = 9419
  {{- end }}
  {{- if .Values.memoryHighWatermark.enabled }}
  ## Memory Threshold
  total_memory_available_override_value = {{ include "rabbitmq.toBytes" .Values.resources.limits.memory }}
  vm_memory_high_watermark.{{ .Values.memoryHighWatermark.type }} = {{ .Values.memoryHighWatermark.value }}
  {{- end }}
## Configuration file content: extra configuration
## Use this instead of `configuration` to add more configuration
##
extraConfiguration: |-
  #default_vhost = {{ .Release.Namespace }}-vhost
  #disk_free_limit.absolute = 50MB
  #load_definitions = /app/load_definition.json
advancedConfiguration: |-
ldap:
  enabled: false
  servers: []
  port: '389'
  user_dn_pattern: cn=${username},dc=example,dc=org
  tls:
    ## If you enabled TLS/SSL you can set advaced options using the advancedConfiguration parameter.
    ##
    enabled: false
extraVolumeMounts: []
extraVolumes: []
extraSecrets: {}
replicaCount: 2
podManagementPolicy: OrderedReady
podAntiAffinityPreset: soft
podSecurityContext:
  fsGroup: 1001
  runAsUser: 1001
resources:
  limits:
    cpu: 250m
    memory: 500Mi
  requests:
    cpu: 250m
    memory: 500Mi
livenessProbe:
  enabled: true
  initialDelaySeconds: 120
  timeoutSeconds: 20
  periodSeconds: 30
  failureThreshold: 3
  successThreshold: 1
readinessProbe:
  enabled: true
  initialDelaySeconds: 10
  timeoutSeconds: 20
  periodSeconds: 30
  failureThreshold: 3
  successThreshold: 1
serviceAccount:
  create: true
rbac:
  create: true
persistence:
  enabled: true
  selector: {}
  accessMode: ReadWriteOnce
  size: 1Gi
  volumes:
pdb:
  create: true
  ## Min number of pods that must still be available after the eviction
  minAvailable: 1
networkPolicy:
  enabled: true
  allowExternal: false
service:
  type: ClusterIP
  port: 5672
  portName: amqp
  tlsPort: 5671
  tlsPortName: amqp-ssl
  distPort: 25672
  distPortName: dist
  managerPort: 15672
  managerPortName: http-stats
  metricsPort: 9419
  epmdPortName: epmd
ingress:
  enabled: false
  path: /
  certManager: false
  hostname: rabbitmq.local
  tls: false
  secrets: []
metrics:
  enabled: true
  plugins: 'rabbitmq_prometheus'
  podAnnotations:
    prometheus.io/scrape: 'true'
    prometheus.io/port: '{{ .Values.service.metricsPort }}'

  ## Prometheus Service Monitor
  ## ref: https://github.com/coreos/prometheus-operator
  serviceMonitor:
    ## If the operator is installed in your cluster, set to true to create a Service Monitor Entry
    ##
    enabled: true
    interval: 30s
    honorLabels: true
    ## Specify the release for ServiceMonitor. Sometimes it should be custom for prometheus operator to work
    ##
    release: "prometheus"
    ## Used to pass Labels that are used by the Prometheus installed in your cluster to select Service Monitors to work with
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
    ##
    additionalLabels:
      release: prometheus
  prometheusRule:
    enabled: false
    additionalLabels: {}
    namespace: 'default'
    ## List of rules, used as template by Helm.
    ## These are just examples rules inspired from https://awesome-prometheus-alerts.grep.to/rules.html
    # rules:
    #   - alert: RabbitmqDown
    #     expr: rabbitmq_up{service="{{ template "rabbitmq.fullname" . }}"} == 0
    #     for: 5m
    #     labels:
    #       severity: error
    #     annotations:
    #       summary: Rabbitmq down (instance {{ "{{ $labels.instance }}" }})
    #       description: RabbitMQ node down
    #   - alert: ClusterDown
    #     expr: |
    #       sum(rabbitmq_running{service="{{ template "rabbitmq.fullname" . }}"})
    #       < {{ .Values.replicaCount }}
    #     for: 5m
    #     labels:
    #       severity: error
    #     annotations:
    #       summary: Cluster down (instance {{ "{{ $labels.instance }}" }})
    #       description: |
    #           Less than {{ .Values.replicaCount }} nodes running in RabbitMQ cluster
    #           VALUE = {{ "{{ $value }}" }}
    #   - alert: ClusterPartition
    #     expr: rabbitmq_partitions{service="{{ template "rabbitmq.fullname" . }}"} > 0
    #     for: 5m
    #     labels:
    #       severity: error
    #     annotations:
    #       summary: Cluster partition (instance {{ "{{ $labels.instance }}" }})
    #       description: |
    #           Cluster partition
    #           VALUE = {{ "{{ $value }}" }}
    #   - alert: OutOfMemory
    #     expr: |
    #       rabbitmq_node_mem_used{service="{{ template "rabbitmq.fullname" . }}"}
    #       / rabbitmq_node_mem_limit{service="{{ template "rabbitmq.fullname" . }}"}
    #       * 100 > 90
    #     for: 5m
    #     labels:
    #       severity: warning
    #     annotations:
    #       summary: Out of memory (instance {{ "{{ $labels.instance }}" }})
    #       description: |
    #           Memory available for RabbmitMQ is low (< 10%)\n  VALUE = {{ "{{ $value }}" }}
    #           LABELS: {{ "{{ $labels }}" }}
    #   - alert: TooManyConnections
    #     expr: rabbitmq_connectionsTotal{service="{{ template "rabbitmq.fullname" . }}"} > 1000
    #     for: 5m
    #     labels:
    #       severity: warning
    #     annotations:
    #       summary: Too many connections (instance {{ "{{ $labels.instance }}" }})
    #       description: |
    #           RabbitMQ instance has too many connections (> 1000)
    #           VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}
    rules: []
volumePermissions:
  enabled: false
  image:
    registry: docker.io
    repository: bitnami/minideb
    tag: buster
    pullPolicy: Always
    pullSecrets: []
  resources:
    limits: {}
    #   cpu: 100m
    #   memory: 128Mi
    requests: {}
    #   cpu: 100m
    #   memory: 128Mi